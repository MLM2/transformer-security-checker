# ğŸ›¡ï¸ Transformer Security Checker

This project demonstrates how to evaluate the robustness of transformer models (like BERT) against adversarial attacks using the `textattack` library.

## ğŸ“ Structure

- `notebook/adversarial_demo.ipynb` â€” Interactive Jupyter Notebook walkthrough
- `scripts/run_attacks.py` â€” Scripted attack simulation
- `requirements.txt` â€” Install dependencies

## ğŸš€ Usage

1. Install dependencies:
```
pip install -r requirements.txt
```

2. Run attack demo:
```
python scripts/run_attacks.py
```

## ğŸ§ª Tested Models
- BERT (`textattack/bert-base-uncased-imdb`)

## ğŸ“š Dataset
- IMDB movie reviews (binary classification)

## ğŸ”’ Goal
Showcase vulnerabilities in NLP models to raise awareness of robustness and AI security issues.
